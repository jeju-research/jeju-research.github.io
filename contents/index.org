#+TITLE: 연구 정리 
#+AUTHOR: Holy Frege
#+DESCRIPTION: Org-HTML export made simple.
#+KEYWORDS:  org-mode, export, html, theme, style, css, js, bigblow
#+LANGUAGE:  en
#+OPTIONS:   H:4 toc:t num:1  ^:nil
#+MACRO: color @@html:<font color=></font>@@
#+PROPERTY:  header-args :padline no
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+LATEX_HEADER: \usepackage{kotex}
#+latex_header: \hypersetup{colorlinks=true}
#+MACRO: color @@latex:{\color{}@@@@latex:}@@
* ☛ TODO 준비해야 할것들 [0/6]
  - [-] 논문 읽기 [0/4]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/Markov_Decision_Policies.pdf][Markov Decision Policies for Dynamic Video Delivery in Wireless Caching Networks]] 
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/Faderated_Learning.pdf][Faderated Learning:]]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/CST2021_Artificial_Intelligence_for_Wireless_Caching_Schemes_Performance_and_Challenges.pdf][CST2021_Artificial_Intelligence_for_Wireless...]]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/fedmes_speeding_up_federated.pdf][FedMes_Speeding_Up_Federated_Learning...]]
- [-] [[file:Faderated_Learning/FL.org][Faderated Learning(Raspberry pi or Arduino)]]
- [-] [[file:Deep_Learning/Deep_Learning.org][to study python, js or c# for platform of Reinforcement Learning.]]
- [-] [[file:Deep_Learning/Deep_Learning.org][tensorflow2, keras, pandas for deep learning.]]
- [-] [[file:Network/network.org][network (무선캐싱,콘텐츠전송, 네트워크 자원 최적화, 에지 컴퓨팅,에지 네트워크 및 분산 시스템)]]
- [-] [[file:Multi_Bandits/mbandits.org][multi bandits code implementations.]]

  - [[file:Papers/papers.org][papers summary link]]
* <2021-11-30 Tue>
 #+begin_important
간단히 git을 만들고, web page를 만들었다.

 #+end_important

** MP와 MDP의 차이
#+begin_note
일상 생활에서 MP와 MDP 적용여부를 알 수 있을까?
#+end_note

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin1.png
- MDP로 설계할 수 있다. (공을 차는 선수를 agent로...) 공이나, 골키퍼등은 상태로 처리할 수 있다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin2.png]]
- +agent인 선수를 고려하지 않고 공만 보자.  공의 경우, MDP를 따르게 할 수 있는가? 할 수 없다. 하지만 MP or MRP를 따를 수 있다. action이 없기 때문이다.+
- +공에 action을 주어서, 예를 들면, 동서 남북으로 갈수 있는 action이 있다고 가정하고, 그 값을 0으로 해서 처리할 수 있을까? 즉, 돌이나 공같은 무생물에+
  +action을 부여할 수 있는가? 없다. action의 값은 random하고 policy로 정해지는데, policy는 확률이라서 모든 action값을 0으로 부여하면 policy가 성립되지 않는다.+
- action이 없기 때문에 MRP나 MP로 설계한다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship1.png]]
- 종이배는 MP와 MRP를 따르게 설계할 수 있다. Reward와 Transition Matrix를 적용해서 종이배의 다음위치를 예측은 가능하다. 하지만, MDP는 안된다.  모터를 달면 MDP가 가능하다.

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship2.png
-  Motor가 있기 때문에,MDP를 적용할 수 있다.

* <2021-12-01 Wed> - chapter2
** 강화학습 기초1: MDP와 벨만 방정식
- 강화학습: 결국엔 어떤 방정식을 풀어내는 방법; 그 방정식은 벨만 방정식
- 순차적 행동 결정 문제(MDP) in grid world.
- MDP를 통해 정의된문제: agent가 학습하기 위해서 가치함수 개념도입. (가치함수 \approx 벨만방정식)
*** MDP
| 상태 | 행동 | 보상함수 | 상태변환확률 | 할인률 |
- 문제를 접하다. -> 해결하기 위해선, 문제를 파악(정의)를 해야 한다.
- 사람은 문제를 정의하는데 아무런 문제가 없다. agent는 사람이 문제를 정의해줘야 한다. 순차적 의사결정을 해야하는 agent에게 사용자가 MDP를 만들어 줘야 한다.
*** 상태(S)
- S: agent가 관찰가능한 상태의 집합; "자신의 상황에 대한 관찰",  ex) 로봇의 경우 센서값
- "내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?"라는 질문을 해야 한다.
- S example in Grid World! \\
 \begin{equation}
 \mathcal{S} = \{ ( x_{1}, y_{1} ) , ( x_{2}, y_{2} ) , ( x_{3}, y_{3} ) , ( x_{4}, y_{4} ) , ( x_{5}, y_{5} ) \}
 \end{equation}
#+CAPTION: 
#+NAME: 
#+attr_html: :width 400px :height 300px
#+attr_latex: :width 100px
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/gridworld.jpg
- location 좌표를 상태로 정했다. 25개의 상태가 주어진다. 이것을 집합으로 나타낸게 $\mathcal{S}$ 다.
- $\mathcal{S} = { (1,1),(1,2),(1,3), \ldots , (5,5) }$
- Random Variable $\mathcal{S}_{t} = (1,3)$ ; 확률변수는 대문자로 쓴다.

#+begin_note
 상태라는 단어보단, 상황이란 단어가 더 와닿는다. 하지만, 대부분 상태로 쓰기 때문에 난 상황과 상태를 섞어서 쓸것이다..
상태는 사용자가 정의한다. 이말은 사용자에게 선택권한이 있다고 돌려 말할 수도 있을것같다. 예를 들면,  축구선수란 agent를 만든다고 하자. 나는 상태를 만들어야 한다.  상태가 될 수 있는것은 여러가지가 있을 수 있다. 운동장을 쪼개서 grid형태로 해서 location(위치)를 나타내는것을 상태로 할수 있고,.공의 위치도 상태가 될수 있다.  동료 선수들의 위치도 상태가 될 수 있다. 심지어  잔디밭의 상태, 온도, 습도, 여러개가 있을 수 있다. 그런데, 나는 선택해야 한다. 가장 중요하다고 생각하는 것들을 가지고...그리고 S라는 유한 집합을 만들어야 한다.
#+end_note
*** 행동(A)
- agent가 상태 $\mathcal{S}_{t}$ 에서 할 수 있는 가능한 행동의 집합을 $\mathcal{A}$ 라고 한다.
- *보통 agent가 할수 있는 행동은 모든 상태에서 같다*
- 수식: $\mathcal{A}_{t} = a$ ; A는 random variable
- grid world: A = {up,down,left,right}

 #+begin_important
 Random Variable은 보통 함수로 설명한다. mapping되는 값이 R(실수)이다. 그런데,  위에서 A, S를 RV라고 하면서, 실수값을 갖지 않는다. 하지만, 난... 이 표현들이 의미적으로는 아무 문제가 없다고 생각한다.
 #+end_important
*** 보상함수(r)
- agent가 학습을 할수 있는 유일한 정보. 환경이 agent에게 주는 정보.
- t단계에서 action을 취하면 t+1에서 보상을 받는다는 것이 수식에 나와있음을 참조하자.
- 수식: $\mathcal{r} (s , a) = \mathbb{E} [ \mathcal{R}_{t+1} | \mathcal{S}_{t} = s, \mathcal{A}_{t} = a ]$
- 보상에서 기댓값을 쓰는 이유? 보상을 agent에게 주는 것은 환경이고, 환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄수도 있다. 따라서 이 모든것을 고려해서 보상함수를 기댓값으로 표현한다.
*** 상태변환확률
- 어떤 상태에서 어떤 action이 주어질때, 다음 상태로 가는 것을 확률로 표현할 수 있다.
- $\mathcal{P}_{ss'}^{a} = \mathcal{P} [ \mathcal{S}_{t+1} = s' | \mathcal{S}_{t} = s' , \mathcal{A}_{t} = a ]$
- 위의상태변환확률은 환경의 model이라고 부른다. 환경의 일부다. 
#+begin_note
같은 상태에서 같은  action을 취하더라도 보상이 달라진다는 말을 유념해야 할듯하다. 보상은 환경에서 주어진다는 말도 유념해야 할것 같다. 주사위를 던질때, 어떤 값이 나올지 모르듯이, 보상도 모르기 때문이다. *즉 상태와 action이 주어진다는 것은 마치 주사위를 던지는 것으로 비유해도 될듯하다.*
그리고 상태변환확률은 확률분포로 봐도 된다. 이것도 상태와 action이 주어지는 것을 주사위를 던지는 것으로 비유하고, 3이 나올 확률은? 1이 나올 확률은? 하고 묻는것과 비슷하다.
#+end_note
*** 할인율(discount factor) - $\gamma$
- 보상이 현 시점에서 주어지지 않고, 먼 미래에 주어진다면, 그 가치는 동등할까?
- 복권 당첨시, 당장 1억의 보상을 받는 것과 10년 뒤 1억을 받는 것의 가치는 동등한가?
- 같은 보상이라도 시간에 따라 가치가 달라진다. 이것을 수학적으로 표현한게  할인율이다.
- 할인율의 정의: $\gamma \in [0,1]$
- 미래 보상의 현재 가치는 다음과 같이 표현한다. $\gamma^{k-1} \mathcal{R}_{t+k}$

#+begin_note
의문점이, 현시점에서 받게 될 보상이 미래에 동일하게 주어진다는 가정에서 할인율을 설명하는게 맞는지 모르겠다.복권처럼... 왜냐면, 수식을 보면, 미래에 받는 보상은 현재의 보상과 같을 수가 없기 때문이다. 보상자체가 환경에서 random하게 주어질 뿐이다. 즉 미래에 받을 보상도 랜덤할 뿐이다. 보상을 동일하다고 비유한것은 이해가 안간다.
하지만, 뒤에서 복권처럼 계속 설명한다. 즉 상태와 action이 주어질 때 보상은 t+1에 올수도 있지만, 먼 미래에 올수도 있다. 먼 미래에 올 경우 $\gamma$를 사용해서 가치를 조정해주는 그림과 설명이 나온다.
그러면, 보상이란건, t+1에 올수도 t+2에 올수도, t+3에도 올 수 있는 개념인거 같다. 미래에 받는 것만 확실한듯 하다.
#+end_note
* <2021-12-02 Thu> -continued to chapter2
*** 정책
- 모든 상태에서 agent가 해야할 행동
- 각 상태에서 단하나의 행동을 나타낼 수도 있고, 확률적으로 나타낼 수도 있다.
- 강화학습을 통해 학습 해야 하는 것은 최적 정책.
- 최적 정책: 각 상태에서 단 하나의 행동만을 선택.
- 수식: $\pi (a | s) = \mathcal{P} [ \mathcal{A}_{t} = a | \mathcal{S}_{t} = s ]$
#+CAPTION: policy in grid world
#+NAME:
#+attr_html: :width 800px :height 500px
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/policy.png]]

#+begin_note
모든 상태에서 agent가 해야할 행동을 정한게 policy라고 한다.  grid world그림이 policy를 직관적으로 잘 표현한것 같아 이해가 쉽다. 수식만 봤을 땐, 어떤 상태가 주어진다면 그때 취할 수 있는 action을 확률로 나타낸것이라고 이해했는데...왠지 전체적인 그림이 안 떠올랐다 . 앞으로 정책을 생각할 때, 모든 상태를 나열하고, 각 상태별 action을 표시할 필요가 있다고 느꼈다.
#+end_note
- 여기까지가 MDP를 정의한 것이다.
#+CAPTION: 
#+NAME: 
#+attr_latex: :width 100px
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/reinforce.png]]
- agent의 행동은, 앞으로 받을 보상을 기반으로 결정한다. (앞으로 받을 보상을 가치함수라고 부른다.)
- 행동을 하면, 환경은 agent에게 실제 보상과 다음상태를 알려준다.
- 실제 보상을 통해서, 앞으로 받을 보상이 잘못된것을 알게 된다.
- 이것을 통해서, 가치함수와 정책을 바꿔나가는 학습을 하게 된다.
- 어느정도의 학습을 통해, 가장 많은 보상을 받는 정책을 학습할 수 있다.
#+begin_note
[지금까지 정리] 위의 그림 2.9는 매우 유명한 그림이다. 그런데 나는 이 그림보다 아래 Peter Norvig책의 그림이 더 와닿는다. 우리가 만들고자 하는건 agent의 뇌를 만드는 것이다. 그리고 그 뇌는 벨만 방정식으로 되어 있다. 벨만 방정식을 만들기 위해서 우리는 그림과 같은 구조를 정의해야 한다. Enviroment는 agent가 매순간 마주하는(인지하는) 상황(State)을 제공한다. 어떤 상황이 어떻게 agent에게 주어질지는 알수가 없다. 물론 enviroment는 모델이라고 부르는 확률분포함수가 있어서, 랜덤하게 확률에 따라서 State를 agent에게 준다. State와 State Transition Matrix는 environment를 정의하는 것이다. 또한 agent에는 Action이 있다. 여기까지는 그림과 일치하고 자연스러운 묘사이기 때문에 문제가 없다. 그런데 위 그림에는 reward가 있는데, 아래 그림은 reward가 없다. reward는 학습을 위해서 만들어낸 요소다. reward가 없다고 가정하자. 어떤 상황에 직면했을때, agent는 어떤 action을 취할수 있지만, 그것이 맞는건지,아닌건지 좋은건지, 나쁜건지 판단할 수 없다. 어떤 목적을 위해서 agent를 학습시키는 데 있어서 reward가 필요한것이다. 예를들어서, 개훈련을 보자. 개한테 손이라고 말하면 발을 드는 action을 취하게 하고싶다. 개는 여러 action을 취할 수 있다. 그중 발을 들어올리는 action을 취할 때 reward을 주는 식으로 학습을 시키는 것이다. 이것이 reward다. 그러면 할인율?은 어떻게 볼 것인가? 이부분은 아직 이해를 못했다.
#+end_note
#+CAPTION: 
#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/reinforce2.png]]

*** 가치 함수

