#+TITLE: 연구 정리 
#+AUTHOR: Holy Frege
#+DESCRIPTION: Org-HTML export made simple.
#+KEYWORDS:  org-mode, export, html, theme, style, css, js, bigblow
#+LANGUAGE:  en
#+OPTIONS:   H:4 toc:t num:1  ^:nil
#+MACRO: color @@html:<font color=></font>@@
#+PROPERTY:  header-args :padline no
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+LATEX_HEADER: \usepackage{kotex}
#+latex_header: \hypersetup{colorlinks=true}
#+MACRO: color @@latex:{\color{}@@@@latex:}@@
* ☛ TODO 준비해야 할것들 [0/6]
  - [-] 논문 읽기 [0/4]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/Markov_Decision_Policies.pdf][Markov Decision Policies for Dynamic Video Delivery in Wireless Caching Networks]] 
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/Faderated_Learning.pdf][Faderated Learning:]]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/CST2021_Artificial_Intelligence_for_Wireless_Caching_Schemes_Performance_and_Challenges.pdf][CST2021_Artificial_Intelligence_for_Wireless_Caching_Schemes_Performance_and_Challenges]]
    - [-] [[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/documents/fedmes_speeding_up_federated.pdf][FedMes_Speeding_Up_Federated_Learning_with_Multiple_Edge_Servers]]
- [-] [[file:Faderated_Learning/FL.org][Faderated Learning(Raspberry pi or Arduino)]]
- [-] [[file:Deep_Learning/Deep_Learning.org][to study python, js or c# for platform of Reinforcement Learning.]]
- [-] [[file:Deep_Learning/Deep_Learning.org][tensorflow2, keras, pandas for deep learning.]]
- [-] [[file:Network/network.org][network (무선캐싱,콘텐츠전송, 네트워크 자원 최적화, 에지 컴퓨팅,에지 네트워크 및 분산 시스템)]]
- [-] [[file:Multi_Bandits/mbandits.org][multi bandits code implementations.]]

  - [[file:Papers/papers.org][papers summary link]]
* <2021-11-30 Tue>
 #+begin_important
간단히 git을 만들고, web page를 만들었다.

 #+end_important

** MP와 MDP의 차이
#+begin_note
일상 생활에서 MP와 MDP 적용여부를 알 수 있을까?
#+end_note

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin1.png
- MDP로 설계할 수 있다. (공을 차는 선수를 agent로...) 공이나, 골키퍼등은 상태로 처리할 수 있다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin2.png]]
- +agent인 선수를 고려하지 않고 공만 보자.  공의 경우, MDP를 따르게 할 수 있는가? 할 수 없다. 하지만 MP or MRP를 따를 수 있다. action이 없기 때문이다.+
- +공에 action을 주어서, 예를 들면, 동서 남북으로 갈수 있는 action이 있다고 가정하고, 그 값을 0으로 해서 처리할 수 있을까? 즉, 돌이나 공같은 무생물에+
  +action을 부여할 수 있는가? 없다. action의 값은 random하고 policy로 정해지는데, policy는 확률이라서 모든 action값을 0으로 부여하면 policy가 성립되지 않는다.+
- action이 없기 때문에 MRP나 MP로 설계한다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship1.png]]
- 종이배는 MP와 MRP를 따르게 설계할 수 있다. Reward와 Transition Matrix를 적용해서 종이배의 다음위치를 예측은 가능하다. 하지만, MDP는 안된다.  모터를 달면 MDP가 가능하다.

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship2.png
-  Motor가 있기 때문에,MDP를 적용할 수 있다.

* <2021-12-01 Wed> - chapter2
** 강화학습 기초1: MDP와 벨만 방정식
- 강화학습: 결국엔 어떤 방정식을 풀어내는 방법; 그 방정식은 벨만 방정식
- 순차적 행동 결정 문제(MDP) in grid world.
- MDP를 통해 정의된문제: agent가 학습하기 위해서 가치함수 개념도입. (가치함수 \approx 벨만방정식)
*** MDP
| 상태 | 행동 | 보상함수 | 상태변환확률 | 할인률 |
- 문제를 접하다. -> 해결하기 위해선, 문제를 파악(정의)를 해야 한다.
- 사람은 문제를 정의하는데 아무런 문제가 없다. agent는 사람이 문제를 정의해줘야 한다. 순차적 의사결정을 해야하는 agent에게 사용자가 MDP를 만들어 줘야 한다.
*** 상태(S)
- S: agent가 관찰가능한 상태의 집합; "자신의 상황에 대한 관찰",  ex) 로봇의 경우 센서값
- "내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?"라는 질문을 해야 한다.
- S example in Grid World! \\
 \begin{equation}
 \mathcal{S} = \{ ( x_{1}, y_{1} ) , ( x_{2}, y_{2} ) , ( x_{3}, y_{3} ) , ( x_{4}, y_{4} ) , ( x_{5}, y_{5} ) \}
 \end{equation}
#+CAPTION: 
#+NAME: 
#+attr_html: :width 400px :height 300px
#+attr_latex: :width 100px
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/gridworld.jpg
- location 좌표를 상태로 정했다. 25개의 상태가 주어진다. 이것을 집합으로 나타낸게 $\mathcal{S}$ 다.
- $\mathcal{S} = { (1,1),(1,2),(1,3), \ldots , (5,5) }$
- Random Variable $\mathcal{S}_{t} = (1,3)$ ; 확률변수는 대문자로 쓴다.

#+begin_note
상태는 사용자가 정의한다. 이말은 사용자에게 선택권한이 있다고 돌려 말할 수도 있을것같다. 예를 들면,  축구선수란 agent를 만든다고 하자. 나는 상태를 만들어야 한다.  상태가 될 수 있는것은 여러가지가 있을 수 있다. 운동장을 쪼개서 grid형태로 해서 location(위치)를 나타내는것을 상태로 할수 있고,.공의 위치도 상태가 될수 있다.  동료 선수들의 위치도 상태가 될 수 있다. 심지어  잔디밭의 상태, 온도, 습도, 여러개가 있을 수 있다. 그런데, 나는 선택해야 한다. 가장 중요하다고 생각하는 것들을 가지고...그리고 S라는 유한 집합을 만들어야 한다.
#+end_note
*** 행동(A)
- agent가 상태 $\mathcal{S}_{t}$ 에서 할 수 있는 가능한 행동의 집합을 $\mathcal{A}$ 라고 한다.
- *보통 agent가 할수 있는 행동은 모든 상태에서 같다*
- 수식: $\mathcal{A}_{t} = a$ ; A는 random variable
- grid world: A = {up,down,left,right}

 #+begin_important
 Random Variable은 보통 함수로 설명한다. mapping되는 값이 R(실수)이다. 그런데,  위에서 A, S를 RV라고 하면서, 실수값을 갖지 않는다. 하지만, 난... 이 표현들이 의미적으로는 아무 문제가 없다고 생각한다.
 #+end_important
*** 보상함수(r)
- agent가 학습을 할수 있는 유일한 정보. 환경이 agent에게 주는 정보.
- t단계에서 action을 취하면 t+1에서 보상을 받는다는 것이 수식에 나와있음을 참조하자.
- 수식: $\mathcal{r} (s , a) = \mathbb{E} [ \mathcal{R}_{t+1} | \mathcal{S}_{t} = s, \mathcal{A}_{t} = a ]$
- 보상에서 기댓값을 쓰는 이유? 보상을 agent에게 주는 것은 환경이고, 환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄수도 있습니다. 따라서 이 모든것을 고려해서 보상함수를 기댓값으로 표현합니다.
- 어떤 상태에서 어떤 action이 주어질때, 다음 상태로 가는 것을 확률로 표현할 수 있다.
- $\mathcal{P}_{ss'}^{a} = \mathcal{P} [ \mathcal{S}_{t+1} = s' | \mathcal{S}_{t} = s' , \mathcal{A}_{t} = a ]$
- 위의상태변환확률은 환경의 model이라고 부른다. 환경의 일부다. 
#+begin_note
같은 상태에서 같은  action을 취하더라도 보상이 달라진다는 말을 유념해야 할듯하다. 보상은 환경에서 주어진다는 말도 유념해야 할것 같다. 주사위를 던질때, 어떤 값이 나올지 모르듯이, 보상도 모르기 때문이다. *즉 상태와 action이 주어진다는 것은 마치 주사위를 던지는 것으로 비유해도 될듯하다.*
그리고 상태변환확률은 확률분포로 봐도 된다. 이것도 상태와 action이 주어지는 것을 주사위를 던지는 것으로 비유하고, 3이 나올 확률은? 1이 나올 확률은? 하고 묻는것과 비슷하다.
#+end_note
*** 할인율(discount factor) - $\gamma$
- 보상이 현 시점에서 주어지지 않고, 먼 미래에 주어진다면, 그 가치는 동등할까?
- 복권 당첨시, 당장 1억의 보상을 받는 것과 10년 뒤 1억을 받는 것의 가치는 동등한가?
- 같은 보상이라도 시간에 따라 가치가 달라진다. 이것을 수학적으로 표현한게  할인율이다.
- 할인율의 정의: $\gamma \in [0,1]$
- 미래 보상의 현재 가치는 다음과 같이 표현한다. $\gamma^{k-1} \mathcal{R}_{t+k}$

#+begin_note
의문점이, 현시점에서 받게 될 보상이 미래에 동일하게 주어진다는 가정에서 할인율을 설명하는게 맞는지 모르겠다.복권처럼... 왜냐면, 수식을 보면, 미래에 받는 보상은 현재의 보상과 같을 수가 없기 때문이다. 보상자체가 환경에서 random하게 주어질 뿐이다. 즉 미래에 받을 보상도 랜덤할 뿐이다. 보상을 동일하다고 비유한것은 이해가 안간다.
하지만, 뒤에서 복권처럼 계속 설명한다. 즉 상태와 action이 주어질 때 보상은 t+1에 올수도 있지만, 먼 미래에 올수도 있다. 먼 미래에 올 경우 $\gamma$를 사용해서 가치를 조정해주는 그림과 설명이 나온다.
그러면, 보상이란건, t+1에 올수도 t+2에 올수도, t+3에도 올 수 있는 개념인거 같다. 미래에 받는 것만 확실한듯 하다.
#+end_note
* <2021-12-02 Thu> -continued to chapter2
