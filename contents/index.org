#+TITLE: 연구 정리 
#+AUTHOR: Holy Frege
#+DESCRIPTION: Org-HTML export made simple.
#+KEYWORDS:  org-mode, export, html, theme, style, css, js, bigblow
#+LANGUAGE:  en
#+OPTIONS:   H:4 toc:t num:1  ^:nil
#+MACRO: color @@html:<font color=></font>@@
#+PROPERTY:  header-args :padline no
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+LATEX_HEADER: \usepackage{kotex}
#+latex_header: \hypersetup{colorlinks=true}
#+MACRO: color @@latex:{\color{}@@@@latex:}@@
* <2021-11-30 Tue>
 #+begin_important
간단히 git을 만들고, web page를 만들었다.

 #+end_important

** MP와 MDP의 차이
#+begin_note
일상 생활에서 MP와 MDP 적용여부를 알 수 있을까?
#+end_note

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin1.png
- MDP로 설계할 수 있다. (공을 차는 선수를 agent로...) 공이나, 골키퍼등은 상태로 처리할 수 있다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin2.png]]
- +agent인 선수를 고려하지 않고 공만 보자.  공의 경우, MDP를 따르게 할 수 있는가? 할 수 없다. 하지만 MP or MRP를 따를 수 있다. action이 없기 때문이다.+
- +공에 action을 주어서, 예를 들면, 동서 남북으로 갈수 있는 action이 있다고 가정하고, 그 값을 0으로 해서 처리할 수 있을까? 즉, 돌이나 공같은 무생물에+
  +action을 부여할 수 있는가? 없다. action의 값은 random하고 policy로 정해지는데, policy는 확률이라서 모든 action값을 0으로 부여하면 policy가 성립되지 않는다.+
- action이 없기 때문에 MRP나 MP로 설계한다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship1.png]]
- 종이배는 MP와 MRP를 따르게 설계할 수 있다. Reward와 Transition Matrix를 적용해서 종이배의 다음위치를 예측은 가능하다. 하지만, MDP는 안된다.  모터를 달면 MDP가 가능하다.

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship2.png
-  Motor가 있기 때문에,MDP를 적용할 수 있다.


* <2021-12-01 Wed>
** 강화학습 기초1: MDP와 벨만 방정식
- 강화학습: 어떠한 방정식을 풀어내는 방법; 방정식은 벨만 방정식
- 순차적 행동 결정 문제(MDP) in grid world.
- MDP를 통해 정의된문제: agent가 학습하기 위해서 가치함수 개념도입. (가치함수 \approx 벨만방정식)
*** MDP
| 상태 | 행동 | 보상함수 | 상태변환확률 | 할인률 |

  
