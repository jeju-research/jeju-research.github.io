#+TITLE: 연구 정리 
#+AUTHOR: Holy Frege
#+DESCRIPTION: Org-HTML export made simple.
#+KEYWORDS:  org-mode, export, html, theme, style, css, js, bigblow
#+LANGUAGE:  en
#+OPTIONS:   H:4 toc:t num:1  ^:nil
#+MACRO: color @@html:<font color=></font>@@
#+PROPERTY:  header-args :padline no
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+LATEX_HEADER: \usepackage{kotex}
#+latex_header: \hypersetup{colorlinks=true}
#+MACRO: color @@latex:{\color{}@@@@latex:}@@
* ☛ TODO 차근 차근 준비해야 할것들 [2/4]
  - [-] 논문 읽기 [1/3]
    - [X] Markov Decision Policies for Dynamic Video Delivery in Wireless Caching Networks
    - [X] 
    - [X] 
  - [X] to study Arduino or raspberry pi for faderated learning
  - [X] to study python, js or c# for platform of Reinforcement Learning
  - [X] tensorflow, keras, pandas for deep learning 

* <2021-11-30 Tue>
 #+begin_important
간단히 git을 만들고, web page를 만들었다.

 #+end_important

** MP와 MDP의 차이
#+begin_note
일상 생활에서 MP와 MDP 적용여부를 알 수 있을까?
#+end_note

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin1.png
- MDP로 설계할 수 있다. (공을 차는 선수를 agent로...) 공이나, 골키퍼등은 상태로 처리할 수 있다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/flipcoin2.png]]
- +agent인 선수를 고려하지 않고 공만 보자.  공의 경우, MDP를 따르게 할 수 있는가? 할 수 없다. 하지만 MP or MRP를 따를 수 있다. action이 없기 때문이다.+
- +공에 action을 주어서, 예를 들면, 동서 남북으로 갈수 있는 action이 있다고 가정하고, 그 값을 0으로 해서 처리할 수 있을까? 즉, 돌이나 공같은 무생물에+
  +action을 부여할 수 있는가? 없다. action의 값은 random하고 policy로 정해지는데, policy는 확률이라서 모든 action값을 0으로 부여하면 policy가 성립되지 않는다.+
- action이 없기 때문에 MRP나 MP로 설계한다.

#+NAME: 
[[https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship1.png]]
- 종이배는 MP와 MRP를 따르게 설계할 수 있다. Reward와 Transition Matrix를 적용해서 종이배의 다음위치를 예측은 가능하다. 하지만, MDP는 안된다.  모터를 달면 MDP가 가능하다.

#+NAME: 
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/ship2.png
-  Motor가 있기 때문에,MDP를 적용할 수 있다.


* <2021-12-01 Wed> - 책 내용을 요약 ( 내 생각은 note로)
** 강화학습 기초1: MDP와 벨만 방정식
- 강화학습: 결국엔 어떤 방정식을 풀어내는 방법; 그 방정식은 벨만 방정식
- 순차적 행동 결정 문제(MDP) in grid world.
- MDP를 통해 정의된문제: agent가 학습하기 위해서 가치함수 개념도입. (가치함수 \approx 벨만방정식)
*** MDP
| 상태 | 행동 | 보상함수 | 상태변환확률 | 할인률 |
- 문제를 접하다. -> 해결하기 위해선, 문제를 파악(정의)를 해야 한다.
- 사람은 문제를 정의하는데 아무런 문제가 없다. agent는 사람이 문제를 정의해줘야 한다. 순차적 의사결정을 해야하는 agent에게 사용자가 MDP를 만들어 줘야 한다.
*** 상태(S)
- S: agent가 관찰가능한 상태의 집합; "자신의 상황에 대한 관찰",  ex) 로봇의 경우 센서값
- "내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?"라는 질문을 해야 한다.
- S example in Grid World! \\
 \begin{equation}
 \mathcal{S} = { ( x_{1}, y_{1} ) , ( x_{2}, y_{2} ) , ( x_{3}, y_{3} ) , ( x_{4}, y_{4} ) , ( x_{5}, y_{5} ) }
 \end{equation}
#+CAPTION: 
#+NAME: 
#+attr_html: :width 400px :height 300px
#+attr_latex: :width 100px
https://raw.githubusercontent.com/jeju-research/jeju-research.github.io/main/img/gridworld.jpg
- location 좌표를 상태로 정했다. 25개의 상태가 주어진다. 이것을 집합으로 나타낸게 $\mathcal{S}$ 다.
- $\mathcal{S} = { (1,1),(1,2),(1,3), \ldots , (5,5) }$
- Random Variable $\mathcal{S}_{t}$ = (1,3) ; 확률변수는 대문자로 쓴다.

#+begin_note
상태는 사용자가 정의한다. 이말은 사용자에게 선택권한이 있다고 돌려 말할 수도 있을것같다. 예를 들면,  축구선수란 agent를 만든다고 하자. 나는 상태를 만들어야 한다.  상태가 될 수 있는것은 여러가지가 있을 수 있다. 운동장을 쪼개서 grid형태로 해서 location(위치)를 나타내는것을 상태로 할수 있고,.공의 위치도 상태가 될수 있다.  동료 선수들의 위치도 상태가 될 수 있다. 심지어  잔디밭의 상태, 온도, 습도, 여러개가 있을 수 있다. 그런데, 나는 선택해야 한다. 가장 중요하다고 생각하는 것들을 가지고...그리고 S라는 유한 집합을 만들어야 한다.
#+end_note
  
